{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code\n",
    "\n",
    "Mistake corrected here: **radd function was still doing other + self causing a never ending recursion to the same function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "from __future__ import annotations\n",
    "import math\n",
    "\n",
    "class Value:\n",
    "    '''Lowest unit of this activity. Represents a value, on which we will do operations, differentiation etc.\n",
    "       I need to keep track of: Value, operation that created it, parents that led to its creation, differentiation function?\n",
    "    '''\n",
    "    def __init__(self, data: float, name: Optional[str] = '', _op: str = None, _parents: Tuple['Value'] = ()) -> None:\n",
    "        self.data = data\n",
    "        self._backward = lambda : None\n",
    "        self._op = _op\n",
    "        self._parents = _parents\n",
    "        self.name = name\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        '''way to recreate the value object, meant for developers'''\n",
    "        return f'Value(data={self.data}, name={self.name}, op={self._op})'\n",
    "\n",
    "    def __add__(self, other: Value) -> Value:\n",
    "        '''a+b, where a and b are Value objects, a.__add__(b) called'''\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data + other.data, _op='+', _parents=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __mul__(self, other: Value) -> Value:\n",
    "        '''a*b, where a and b are Value objects, a.__mul__(b) called'''\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Value(other)\n",
    "        \n",
    "        out = Value(self.data * other.data, _op='*', _parents=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        '''when b*a called, where b is a scalar, a.__rmul__(b) called.'''\n",
    "        return self * other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        '''when b+a called, where b is a scalar, a.__radd__(b) called.'''\n",
    "        return self + other\n",
    "    \n",
    "    def __pow__(self, other: float):\n",
    "        '''when we call a ** b, a.__pow__(b) is called, where b is a scalar.'''\n",
    "        if not isinstance(other, (int, float)):\n",
    "            raise ValueError(\"Trying to calculate exp on an incompatible data type. Only supporting int and float.\")\n",
    "        out = Value(self.data ** other, _op=f'**{other}', _parents=(self, ))\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other -1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = (2 * self).exp()\n",
    "        out = (x - 1) / (x + 1)\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        '''calling exponent on a.exp() is equivalent to e raised to a'''\n",
    "        out = Value(math.exp(self.data), _op='exp', _parents=(self, ))\n",
    "        def _backward():\n",
    "            self.grad += math.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, other):\n",
    "        '''when we call a / b on 2 value objects, a.__truediv__(b) is called.'''\n",
    "        out = self * (other ** -1)\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def backward(self):\n",
    "        ordering = []\n",
    "        visited = []\n",
    "        def topological_traversal(e):\n",
    "            if e not in visited:\n",
    "                visited.append(e)\n",
    "                if e._parents:\n",
    "                    for parent in e._parents:\n",
    "                        topological_traversal(parent)\n",
    "                ordering.append(e)\n",
    "        topological_traversal(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(ordering):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def traverse(current: Value, nodes=None, edges=None):\n",
    "    '''traverse to build nodes and edges for computational graph'''\n",
    "    if nodes is None:\n",
    "        nodes = []\n",
    "    if edges is None:\n",
    "        edges = []\n",
    "    if current not in nodes:\n",
    "        nodes.append(current)\n",
    "        for parent in current._parents:\n",
    "            edges.append((parent, current))\n",
    "            traverse(parent, nodes, edges)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def build_graph(current: Value):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "    nodes, edges= traverse(current)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name=uid, label=f'{n.name} | data: {n.data: .3f} | grad: {n.grad: .3f}', shape='record')\n",
    "\n",
    "        if n._op:\n",
    "            dot.node(name=uid+n._op, label=n._op)\n",
    "            dot.edge(uid+n._op, uid)\n",
    "\n",
    "    for node1, node2 in edges:\n",
    "        dot.edge(str(id(node1)), str(id(node2))+node2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN From Scratch Continued:\n",
    "\n",
    "8. Implement till step 7 in Pytorch\n",
    "9. Build Neuron\n",
    "10. Build Layer\n",
    "11. Build Neural Networks\n",
    "12. SGD\n",
    "13. Training\n",
    "14. Training with an actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement till step 7 in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    '''Contains weights and biases'''\n",
    "    def __init__(self, n_inputs: int) -> None:\n",
    "        self.weights = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "        self.bias = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert len(x) == len(self.weights), f\"Expecting input of shape {len(self.weights)} but got {len(x)}\"\n",
    "        act = sum(wi * xi for wi, xi in zip(self.weights, x)) + self.bias\n",
    "        out = act.tanh()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m)\n\u001b[1;32m      2\u001b[0m n \u001b[38;5;241m=\u001b[39m Neuron(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting input of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m act\u001b[38;5;241m.\u001b[39mtanh()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36mValue.__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''when b+a called, where b is a scalar, a.__radd__(b) called.'''\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mother\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36mValue.__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''when b+a called, where b is a scalar, a.__radd__(b) called.'''\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mother\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n",
      "    \u001b[0;31m[... skipping similar frames: Value.__radd__ at line 50 (737 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36mValue.__radd__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''when b+a called, where b is a scalar, a.__radd__(b) called.'''\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mother\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "x = (2.0, 3.0)\n",
    "n = Neuron(2)\n",
    "\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
