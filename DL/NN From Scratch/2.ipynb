{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code\n",
    "\n",
    "Mistake corrected here: **radd function was still doing other + self causing a never ending recursion to the same function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, List\n",
    "from __future__ import annotations\n",
    "import math\n",
    "\n",
    "class Value:\n",
    "    '''Lowest unit of this activity. Represents a value, on which we will do operations, differentiation etc.\n",
    "       I need to keep track of: Value, operation that created it, parents that led to its creation, differentiation function?\n",
    "    '''\n",
    "    def __init__(self, data: float, name: Optional[str] = '', _op: str = None, _parents: Tuple['Value'] = ()) -> None:\n",
    "        self.data = data\n",
    "        self._backward = lambda : None\n",
    "        self._op = _op\n",
    "        self._parents = _parents\n",
    "        self.name = name\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        '''way to recreate the value object, meant for developers'''\n",
    "        return f'Value(data={self.data}, name={self.name}, op={self._op})'\n",
    "\n",
    "    def __add__(self, other: Value) -> Value:\n",
    "        '''a+b, where a and b are Value objects, a.__add__(b) called'''\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Value(other)\n",
    "        out = Value(self.data + other.data, _op='+', _parents=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __mul__(self, other: Value) -> Value:\n",
    "        '''a*b, where a and b are Value objects, a.__mul__(b) called'''\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Value(other)\n",
    "        \n",
    "        out = Value(self.data * other.data, _op='*', _parents=(self, other))\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        '''when b*a called, where b is a scalar, a.__rmul__(b) called.'''\n",
    "        return self * other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        '''when b+a called, where b is a scalar, a.__radd__(b) called.'''\n",
    "        return self + other\n",
    "    \n",
    "    def __pow__(self, other: float):\n",
    "        '''when we call a ** b, a.__pow__(b) is called, where b is a scalar.'''\n",
    "        if not isinstance(other, (int, float)):\n",
    "            raise ValueError(\"Trying to calculate exp on an incompatible data type. Only supporting int and float.\")\n",
    "        out = Value(self.data ** other, _op=f'**{other}', _parents=(self, ))\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other -1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = (2 * self).exp()\n",
    "        out = (x - 1) / (x + 1)\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        '''calling exponent on a.exp() is equivalent to e raised to a'''\n",
    "        out = Value(math.exp(self.data), _op='exp', _parents=(self, ))\n",
    "        def _backward():\n",
    "            self.grad += math.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __truediv__(self, other):\n",
    "        '''when we call a / b on 2 value objects, a.__truediv__(b) is called.'''\n",
    "        out = self * (other ** -1)\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def backward(self):\n",
    "        ordering = []\n",
    "        visited = []\n",
    "        def topological_traversal(e):\n",
    "            if e not in visited:\n",
    "                visited.append(e)\n",
    "                if e._parents:\n",
    "                    for parent in e._parents:\n",
    "                        topological_traversal(parent)\n",
    "                ordering.append(e)\n",
    "        topological_traversal(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(ordering):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def traverse(current: Value, nodes=None, edges=None):\n",
    "    '''traverse to build nodes and edges for computational graph'''\n",
    "    if nodes is None:\n",
    "        nodes = []\n",
    "    if edges is None:\n",
    "        edges = []\n",
    "    if current not in nodes:\n",
    "        nodes.append(current)\n",
    "        for parent in current._parents:\n",
    "            edges.append((parent, current))\n",
    "            traverse(parent, nodes, edges)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def build_graph(current: Value):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "\n",
    "    nodes, edges= traverse(current)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        dot.node(name=uid, label=f'{n.name} | data: {n.data: .3f} | grad: {n.grad: .3f}', shape='record')\n",
    "\n",
    "        if n._op:\n",
    "            dot.node(name=uid+n._op, label=n._op)\n",
    "            dot.edge(uid+n._op, uid)\n",
    "\n",
    "    for node1, node2 in edges:\n",
    "        dot.edge(str(id(node1)), str(id(node2))+node2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN From Scratch Continued:\n",
    "\n",
    "8. Implement till step 7 in Pytorch\n",
    "9. Build Neuron\n",
    "10. Build Layer\n",
    "11. Build Neural Networks\n",
    "12. SGD\n",
    "13. Training\n",
    "14. Training with an actual dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement till step 7 in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import random\n",
    "\n",
    "class Module(ABC):\n",
    "    @abstractmethod\n",
    "    def get_parameters(self) -> List:\n",
    "        return []\n",
    "\n",
    "    @abstractmethod\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.get_parameters():\n",
    "            param.grad = 0\n",
    "\n",
    "class Neuron(Module):\n",
    "    '''Contains weights and biases'''\n",
    "    def __init__(self, n_inputs: int) -> None:\n",
    "        self.weights = [Value(random.uniform(-1, 1)) for _ in range(n_inputs)]\n",
    "        self.bias = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        assert len(x) == len(self.weights), f\"Expecting input of shape {len(self.weights)} but got {len(x)}\"\n",
    "        act = sum(wi * xi for wi, xi in zip(self.weights, x)) + self.bias\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def get_parameters(self) -> List:\n",
    "        return self.weights + [self.bias]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return super().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module): \n",
    "    '''Collection of neurons form a layer in a network'''\n",
    "    def __init__(self, n_inputs: int, n_outputs: int) -> None:\n",
    "        '''Define a collection of neurons\n",
    "        \n",
    "        Args:\n",
    "            n_inputs: Number of inputs to each neuron\n",
    "            n_outputs: Number of outputs i.e. number of neurons since there is 1 output per neuron\n",
    "        '''\n",
    "        self.neurons = [Neuron(n_inputs) for _ in range(n_outputs)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        outputs = [neuron(x) for neuron in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return [param for neuron in self.neurons for param in neuron.get_parameters()]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return super().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.7507831924131323, name=, op=*),\n",
       " Value(data=0.13729160250852201, name=, op=*),\n",
       " Value(data=0.9359463339495423, name=, op=*)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "n = Layer(2, 3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, n_inputs: float, n_outputs: List[float]):\n",
    "        n_neurons = [n_inputs] + n_outputs\n",
    "        self.layers = [Layer(n_neurons[i], n_neurons[i+1]) for i in range(len(n_outputs))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''One forward pass through the network with a set of inputs?'''\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_parameters(self) -> List:\n",
    "        return [param for layer in self.layers for param in layer.get_parameters()]\n",
    "\n",
    "    def zero_grad(self):\n",
    "            return super().zero_grad()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step():\n",
    "        pass\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LearningRate:\n",
    "    learning_rate: float = 0.0\n",
    "\n",
    "\n",
    "class LRScheduler(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self):\n",
    "        return []\n",
    "\n",
    "class LRScheduler():\n",
    "    pass\n",
    "\n",
    "class Loss:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Loss Value(data=7.118754135868953, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=-0.7778217722722601, name=, op=*), Value(data=-0.3747675889074075, name=, op=*), Value(data=-0.7780027159923555, name=, op=*), Value(data=-0.3629013765932132, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=6.873059903063769, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=-0.7525866833363184, name=, op=*), Value(data=-0.4014010903361636, name=, op=*), Value(data=-0.7527545646701249, name=, op=*), Value(data=-0.3908771865762381, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=6.345018420808577, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=-0.690679648804102, name=, op=*), Value(data=-0.44376893129909106, name=, op=*), Value(data=-0.6908025550469507, name=, op=*), Value(data=-0.4357177443984593, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=5.4113431368405145, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=-0.5613401788860024, name=, op=*), Value(data=-0.4847813435691719, name=, op=*), Value(data=-0.5613243975244542, name=, op=*), Value(data=-0.48002322705984946, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=3.879396792967911, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=-0.3026709021152857, name=, op=*), Value(data=-0.5071837488806453, name=, op=*), Value(data=-0.3022794668413391, name=, op=*), Value(data=-0.5063952526300401, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=2.005121136973673, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.1305828231237284, name=, op=*), Value(data=-0.5004178441867096, name=, op=*), Value(data=0.13160244969544324, name=, op=*), Value(data=-0.5044818129027387, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.8971496626616693, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.5761869437252165, name=, op=*), Value(data=-0.47601760222228995, name=, op=*), Value(data=0.5774090463777748, name=, op=*), Value(data=-0.4858098710316617, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.6063811540831937, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.8319256914559018, name=, op=*), Value(data=-0.46797455505602126, name=, op=*), Value(data=0.8327667215400816, name=, op=*), Value(data=-0.48316914043535886, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.47799133179451625, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9373574073458802, name=, op=*), Value(data=-0.5061425229716483, name=, op=*), Value(data=0.9378047098235824, name=, op=*), Value(data=-0.5242860358964133, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.3120002052116021, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9765975999068377, name=, op=*), Value(data=-0.5973250497270541, name=, op=*), Value(data=0.9768127106456834, name=, op=*), Value(data=-0.6142957520112574, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.1572513218482155, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9911154946627383, name=, op=*), Value(data=-0.7135384947037281, name=, op=*), Value(data=0.9912144787542285, name=, op=*), Value(data=-0.7260748131095613, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.06439465114193471, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9965822289049932, name=, op=*), Value(data=-0.8168179250485442, name=, op=*), Value(data=0.9966265922186662, name=, op=*), Value(data=-0.8244553688684928, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.023171014864316372, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.998672520357282, name=, op=*), Value(data=-0.8903523174477095, name=, op=*), Value(data=0.998692021354016, name=, op=*), Value(data=-0.8944304609989779, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.007778661230582912, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9994799464507879, name=, op=*), Value(data=-0.9366425801470979, name=, op=*), Value(data=0.9994883940708, name=, op=*), Value(data=-0.938648827363732, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.0025259775511012723, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9997943598894427, name=, op=*), Value(data=-0.9639955725515913, name=, op=*), Value(data=0.9997979814542747, name=, op=*), Value(data=-0.9649346944136279, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.0008094729462774087, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9999178038979628, name=, op=*), Value(data=-0.9796700613765061, name=, op=*), Value(data=0.999919346482892, name=, op=*), Value(data=-0.9800964003091559, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=0.00025872196905847003, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9999667281954888, name=, op=*), Value(data=-0.9885318676678233, name=, op=*), Value(data=0.9999673834761575, name=, op=*), Value(data=-0.9887216251605317, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=8.291967529820385e-05, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9999863333618842, name=, op=*), Value(data=-0.9935195696719602, name=, op=*), Value(data=0.9999866120176809, name=, op=*), Value(data=-0.9936028653252759, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=2.671553532963395e-05, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9999942914382779, name=, op=*), Value(data=-0.9963271390213881, name=, op=*), Value(data=0.9999944105301247, name=, op=*), Value(data=-0.9963633031839905, name=, op=*)]\n",
      "********************\n",
      "Loss Value(data=8.661225705511132e-06, name=, op=+)\n",
      "\n",
      "\n",
      "[Value(data=0.9999975700592074, name=, op=*), Value(data=-0.9979112311863692, name=, op=*), Value(data=0.9999976214293601, name=, op=*), Value(data=-0.9979267757027992, name=, op=*)]\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "    [0.5, 0.7, -1.0, 1.0],\n",
    "    [0.1, 0.4, 1.0, -1.0],\n",
    "    [0.3, 0.7, -1.0, 0.9],\n",
    "    [0.1, 0.2, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "y_true = [1.0, -1.0, 1.0, -1.0]\n",
    "nn = MLP(4, [3, 3, 1])\n",
    "learning_rate = 0.01\n",
    "\n",
    "from tqdm import tqdm\n",
    "for _ in range(20):\n",
    "    ypred = [nn(xa) for xa in x]\n",
    "    squared_error_loss = sum((pred - true)**2 for true, pred  in zip(y_true, ypred))\n",
    "    parameter_list = nn.get_parameters()\n",
    "    squared_error_loss.backward()\n",
    "\n",
    "    for parameter in parameter_list:\n",
    "        parameter.data -= 0.01 * parameter.grad\n",
    "\n",
    "    print('*'*20)\n",
    "    print('Loss', squared_error_loss)\n",
    "    print('\\n')\n",
    "    print(ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
